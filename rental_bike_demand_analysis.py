# -*- coding: utf-8 -*-
"""Rental Bike Demand Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Geo36Pk2rVfoA34ZfjvQCsP4gSDNa-9N

## Project Title: Predicting Analysis for Rental Bike Demand
### Maheen Adeeb

# Phase 1: Data Integration, Data Cleaning & EDA

# Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error

"""# Getting Insights of Datasets"""

day_df = pd.read_csv('/day.csv')
hour_df = pd.read_csv('/hour.csv')
print(day_df)
print(hour_df)

"""# Working on only hourly datasets

### Descriptive Analysis
"""

hour_df.shape

hour_df.head()

# Shpwing some Statistics information about datasets
hour_df.describe()

# Cheking null values

hour_df.isnull().sum()

"""## Changing column name for better understanding"""

# Rename some columns
print("Column Names Before\n")
print(hour_df.columns)
hour_df = hour_df.rename(columns={'yr':'year', 'dteday':'date',
                                            'mnth':'month',
                                            'hum':'humidity',
                                            'weathersit':'weather',
                                            'cnt':'count',
                                           'hr':'hour'})
# Show dataset columns after rename some columns
print("\n")
print("Column Names After renaming\n")
print(hour_df.columns)

hour_df.nunique()

hour_df['season'].nunique()

"""# Data Visualization"""

# Group data by hour and plot
hourly_rentals = hour_df.groupby('hour')['count'].mean()
sns.lineplot(x=hourly_rentals.index, y=hourly_rentals.values)
plt.title('Average Bike Rentals by Hour of the Day')
plt.xlabel('Hour of the Day')
plt.ylabel('Average Count of Rentals')
plt.show()

"""# Bike Rentals by Season"""

season_mapping = {1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'}
hour_df['season'] = hour_df['season'].map(season_mapping)

season_rentals = hour_df.groupby('season')['count'].mean()
sns.barplot(x=season_rentals.index, y=season_rentals.values)
plt.title('Average Bike Rentals by Season')
plt.xlabel('Season')
plt.ylabel('Average Count of Rentals')
plt.show()

"""# Bike Rentals by Weather Condition"""

weathersit_mapping = {1: 'Clear', 2: 'Mist', 3: 'Light Snow/Rain', 4: 'Heavy Rain/Fog'}
hour_df['weather'] = hour_df['weather'].map(weathersit_mapping)

weather_rentals = hour_df.groupby('weather')['count'].mean()
sns.barplot(x=weather_rentals.index, y=weather_rentals.values)
plt.title('Average Bike Rentals by Weather Condition')
plt.xlabel('Weather Condition')
plt.ylabel('Average Count of Rentals')
plt.show()

"""# Bike Rentals vs Temperature"""

sns.scatterplot(x='temp', y='count', data=hour_df, alpha=0.5)
plt.title('Bike Rentals vs. Temperature')
plt.xlabel('Normalized Temperature')
plt.ylabel('Count of Rentals')
plt.show()

"""# Bike Rentals on Holidays Vs. Working Days"""

sns.boxplot(x='holiday', y='count', data=hour_df)
plt.title('Bike Rentals on Holidays vs. Non-Holidays')
plt.xlabel('Holiday (0 = No, 1 = Yes)')
plt.ylabel('Count of Rentals')
plt.show()

"""# Bike Rental during Weekends and Weekdays"""

weekday_mapping = {0: 'Sunday', 1: 'Monday', 2: 'Tuesday', 3: 'Wednesday',
                   4: 'Thursday', 5: 'Friday', 6: 'Saturday'}

hour_df['weekday_name'] = hour_df['weekday'].map(weekday_mapping)

# Plot the bike rental counts during weekends and weekdays
sns.pointplot(data=hour_df, x='hour', y='count', hue='weekday_name')
plt.title('Count During Weekends and Weekdays')
plt.show()

"""# Correlation Heatmap"""

corr_matrix = hour_df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Variables')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error

# Aggregate daily bike rentals by month
monthly_rentals = day_df.groupby('mnth')['cnt'].sum().reset_index()

# Map month numbers to names for better visualization
monthly_rentals['mnth'] = monthly_rentals['mnth'].map({
    1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May',
    6: 'June', 7: 'July', 8: 'August', 9: 'September', 10: 'October',
    11: 'November', 12: 'December'
})

# Sort months for proper ordering in the plot
monthly_rentals['mnth'] = pd.Categorical(monthly_rentals['mnth'], categories=monthly_rentals['mnth'].tolist(), ordered=True)
monthly_rentals = monthly_rentals.sort_values('mnth')

# Plot monthly bike rentals
plt.figure(figsize=(12, 6))
plt.bar(monthly_rentals['mnth'], monthly_rentals['cnt'], color='salmon')
plt.title('Total Bike Rentals by Month')
plt.xlabel('Month')
plt.ylabel('Total Rentals')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Aggregate daily bike rentals by weekday
weekday_rentals = day_df.groupby('weekday')['cnt'].sum().reset_index()

# Map weekday numbers to names
weekday_map = {0: 'Sunday', 1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', 4: 'Thursday', 5: 'Friday', 6: 'Saturday'}
weekday_rentals['weekday'] = weekday_rentals['weekday'].map(weekday_map)

# Plot rentals by weekday
plt.figure(figsize=(10, 6))
plt.bar(weekday_rentals['weekday'], weekday_rentals['cnt'], color='coral')
plt.title('Total Bike Rentals by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Total Rentals')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Convert 'dteday' to datetime format and plot
day_df['dteday'] = pd.to_datetime(day_df['dteday'])

plt.figure(figsize=(14, 6))
plt.plot(day_df['dteday'], day_df['cnt'], marker='o', linestyle='-', color='b')
plt.title('Daily Bike Rentals Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Rentals')
plt.xticks(rotation=45)
plt.grid()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Set 'dteday' as the index
day_df.set_index('dteday', inplace=True)

# Ensure that the data is continuous
day_df = day_df.resample('D').sum()  # Resample to fill in any missing dates

# Set frequency
day_df.index.freq = 'D'  # Daily frequency

# Decompose the time series
decomposition = seasonal_decompose(day_df['cnt'], model='additive')
fig = decomposition.plot()
plt.tight_layout()
plt.show()

"""The data displays fluctuations, with observable peaks and troughs. You can notice periods of high rentals and low rentals, reflecting seasonal trends or irregular events.

The trend appears to show a general increase over time, suggesting that bike rentals are growing. However, there are some fluctuations, especially toward the end of the series, indicating a potential downward shift.

The seasonal component displays clear periodic peaks and troughs, indicating that bike rentals have consistent seasonal behavior. For instance, the rentals peak during certain months, likely in spring and summer, and drop during winter.

The residuals should ideally appear as a random scatter around zero. In your case, the residuals show some variability but generally fluctuate within a range, suggesting that there are random effects influencing bike rentals not accounted for by the trend and seasonality.

**Model Building**
"""

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Load the data
day_df = pd.read_csv('/day.csv')
day_df['dteday'] = pd.to_datetime(day_df['dteday'])
day_df.set_index('dteday', inplace=True)

# Ensure data is sorted by date
day_df.sort_index(inplace=True)

import pandas as pd
from statsmodels.tsa.stattools import adfuller
# Perform ADF Test
result = adfuller(day_df['cnt'])

# Print the results
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

# Apply first differencing
day_df['diff_cnt'] = day_df['cnt'].diff()

# Drop the first NaN value resulting from the differencing
day_df.dropna(inplace=True)

# Re-run the ADF test on the differenced data
result = adfuller(day_df['diff_cnt'])

# Print the results
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
# Plot ACF
plt.figure(figsize=(14, 7))
plot_acf(day_df['diff_cnt'], lags=40, alpha=0.05)  # Adjust lags as needed
plt.title('Autocorrelation Function for Differenced Data')
plt.show()

# Plot PACF
plt.figure(figsize=(14, 7))
plot_pacf(day_df['diff_cnt'], lags=40, alpha=0.05)  # Adjust lags as needed
plt.title('Partial Autocorrelation Function for Differenced Data')
plt.show()

from statsmodels.tsa.arima.model import ARIMA

# Assuming day_data['cnt'] contains the non-differenced original data
model = ARIMA(day_df['cnt'], order=(1, 1, 0))
model_fit = model.fit()

# Display the model summary
print(model_fit.summary())

# Plot diagnostic plots
model_fit.plot_diagnostics(figsize=(12, 8))
plt.show()

import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX

# Example: Assuming day_data is your DataFrame and 'cnt' is the rentals column
# Ensure your date is the DataFrame index in proper datetime format
day_df.index = pd.to_datetime(day_df.index)

# Define a SARIMA Model
# Note: These are hypothetical parameters, adjust based on your ACF and PACF analysis and seasonal period
sarima_model = SARIMAX(day_df['cnt'],
                       order=(1, 1, 1),              # Non-seasonal parameters
                       seasonal_order=(1, 1, 1, 12) # Seasonal parameters: adjust the period (365) based on actual data
                      )

# Fit the model
sarima_result = sarima_model.fit()

# Summarize the model
print(sarima_result.summary())

# Check diagnostics
sarima_result.plot_diagnostics(figsize=(15, 12))
plt.show()

# Forecasting
forecast_steps = 30  # Number of steps to forecast (e.g., 30 days)
forecast = sarima_result.get_forecast(steps=forecast_steps)
forecast_ci = forecast.conf_int()

# Plot the forecast alongside actual data
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(day_df['cnt'], label='Actual')
plt.plot(pd.date_range(day_df.index[-1], periods=forecast_steps, freq='D'), forecast.predicted_mean, label='Forecasted')
plt.fill_between(pd.date_range(day_df.index[-1], periods=forecast_steps, freq='D'), forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='pink', alpha=0.3)
plt.title('Forecast vs Actuals')
plt.legend()
plt.show()

"""# Linear Regression Model"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Convert categorical variables into dummy/indicator variables
X = hour_df[['temp', 'humidity', 'windspeed', 'season', 'hour', 'holiday', 'weekday']]
X = pd.get_dummies(X, columns=['season', 'holiday', 'weekday'], drop_first=True)

y = hour_df['count']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred_lr = lr_model.predict(X_test)
mse_lr = mean_squared_error(y_test, y_pred_lr)
print(f'Linear Regression Mean Squared Error: {mse_lr}')

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_lr, alpha=0.7)
plt.plot([0, max(y_test)], [0, max(y_test)], color='red', linestyle='--')  # Line for perfect predictions
plt.title('Actual vs Predicted Bike Rentals')
plt.xlabel('Actual Rentals')
plt.ylabel('Predicted Rentals')
plt.show()

"""# Testing whether model is overfitting or not"""

# Make predictions on the training set
y_pred_train = lr_model.predict(X_train)

# Make predictions on the test set
y_pred_test = lr_model.predict(X_test)

# Calculate metrics for the training set
mse_train = mean_squared_error(y_train, y_pred_train)
r2_train = r2_score(y_train, y_pred_train)

# Calculate metrics for the test set
mse_test = mean_squared_error(y_test, y_pred_test)
r2_test = r2_score(y_test, y_pred_test)

# Print results
print(f'Training Set Mean Squared Error: {mse_train}')
print(f'Training Set R-squared: {r2_train}')
print(f'Test Set Mean Squared Error: {mse_test}')
print(f'Test Set R-squared: {r2_test}')

# Check for overfitting
if mse_train < mse_test:
    print("The model may be overfitting.")
else:
    print("The model is not overfitting.")

"""# Decision Tree Regressor"""

from sklearn.tree import DecisionTreeRegressor

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Decision Tree Regressor
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_dt = dt_model.predict(X_test)

# Evaluate the model
mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)

print(f'Decision Tree Mean Squared Error: {mse_dt}')
print(f'Decision Tree R-squared: {r2_dt}')

"""# Decision Tree: Grid Search with cross validation"""

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Initialize the Decision Tree Regressor
dt_model = DecisionTreeRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Best model from Grid Search
best_dt_model = grid_search.best_estimator_

# Make predictions with the best model
y_pred_best_dt = best_dt_model.predict(X_test)

# Evaluate the model
mse_best_dt = mean_squared_error(y_test, y_pred_best_dt)
r2_best_dt = r2_score(y_test, y_pred_best_dt)

print(f'Grid Search Best Decision Tree Mean Squared Error: {mse_best_dt}')
print(f'Grid Search Best Decision Tree R-squared: {r2_best_dt}')

"""# Randomized Search with Cross - Validation"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the parameter distribution
param_dist = {
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': randint(2, 11),  # Random integer between 2 and 10
    'min_samples_leaf': randint(1, 5),     # Random integer between 1 and 4
}

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=dt_model, param_distributions=param_dist, n_iter=100, cv=5, scoring='neg_mean_squared_error', random_state=42)
random_search.fit(X_train, y_train)

# Best model from Random Search
best_random_dt_model = random_search.best_estimator_

# Make predictions with the best model
y_pred_random_best_dt = best_random_dt_model.predict(X_test)

# Evaluate the model
mse_random_best_dt = mean_squared_error(y_test, y_pred_random_best_dt)
r2_random_best_dt = r2_score(y_test, y_pred_random_best_dt)

print(f'Randomized Search Best Decision Tree Mean Squared Error: {mse_random_best_dt}')
print(f'Randomized Search Best Decision Tree R-squared: {r2_random_best_dt}')

"""# Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

# Initialize and train the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f'Random Forest Mean Squared Error: {mse_rf}')
print(f'Random Forest R-squared: {r2_rf}')

# Get feature importances
feature_importances_rf = rf_model.feature_importances_

# Create a DataFrame for better visualization
feature_names_rf = X.columns
importance_rf_df = pd.DataFrame({'Feature': feature_names_rf, 'Importance': feature_importances_rf})

# Sort the DataFrame by importance
importance_rf_df = importance_rf_df.sort_values(by='Importance', ascending=False)

# Select the top 10 features
top_features_rf = importance_rf_df.head(10)

# Plotting
plt.barh(top_features_rf['Feature'], top_features_rf['Importance'], color='lightblue', edgecolor='black')
plt.xlabel('Importance', fontsize=12)
plt.title('Top 10 Feature Importances from Random Forest Regressor', fontsize=14)
plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top
plt.show()

"""# Gradient Boosting Model"""

from sklearn.ensemble import GradientBoostingRegressor

gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
mse_gb = mean_squared_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)

# Print results
print(f'Gradient Boosting Mean Squared Error: {mse_gb}')
print(f'Gradient Boosting R-squared: {r2_gb}')

# Get feature importances
feature_importances_rf = gb_model.feature_importances_

# Create a DataFrame for better visualization
feature_names_rf = X.columns
importance_rf_df = pd.DataFrame({'Feature': feature_names_rf, 'Importance': feature_importances_rf})

# Sort the DataFrame by importance
importance_rf_df = importance_rf_df.sort_values(by='Importance', ascending=False)

# Select the top 10 features
top_features_rf = importance_rf_df.head(10)

# Plotting
plt.barh(top_features_rf['Feature'], top_features_rf['Importance'], edgecolor='black')
plt.xlabel('Importance', fontsize=12)
plt.title('Top 10 Feature Importances from Gradient Boosting Regressor', fontsize=14)
plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Build the Neural Network Model
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer
model.add(Dense(32, activation='relu'))  # Hidden layer
model.add(Dense(1))  # Output layer

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# Make predictions to evaluate performance
y_pred_nn = model.predict(X_test)

# Calculate MSE and R-squared
mse_nn = mean_squared_error(y_test, y_pred_nn)
r2_nn = r2_score(y_test, y_pred_nn)

print(f'Neural Network Mean Squared Error: {mse_nn}')
print(f'Neural Network R-squared: {r2_nn}')

plt.scatter(y_test, y_pred_nn, alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Line for perfect predictions
plt.title('Neural Network Predictions vs Actual Bike Rentals')
plt.xlabel('Actual Rentals')
plt.ylabel('Predicted Rentals')
plt.show()

"""# Time series cross validation"""

from sklearn.model_selection import TimeSeriesSplit
# Initialize TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

# Initialize model (e.g., Gradient Boosting Regressor)
model = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Lists to store results
mse_list = []
r2_list = []

# TimeSeriesSplit loop
split_num = 1
for train_index, test_index in tscv.split(X):
    # Split the data
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Train the model on the current split
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    mse_list.append(mse)
    r2_list.append(r2)

    print(f"Split {split_num}:")
    print(f"Mean Squared Error: {mse}")
    print(f"R-squared: {r2}\n")
    split_num += 1

# Average metrics over all splits
mean_mse = np.mean(mse_list)
mean_r2 = np.mean(r2_list)

print(f"Average Mean Squared Error: {mean_mse}")
print(f"Average R-squared: {mean_r2}")

plt.plot(range(1, len(mse_list) + 1), mse_list, marker='o', label='MSE')
plt.plot(range(1, len(r2_list) + 1), r2_list, marker='o', label='R-squared')
plt.title('Time Series Cross-Validation Performance')
plt.xlabel('Cross-Validation Split')
plt.ylabel('Score')
plt.legend()
plt.show()

